{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_W = 64\n",
    "N_H = 64\n",
    "N_C = 3\n",
    "N_CLASS = 369\n",
    "BATCH_SIZE = 64\n",
    "learn_rate = 0.0001   #学习率\n",
    "pool_size = (2, 2)\n",
    "TRAIN_LEN = 1514097\n",
    "TEST_LEN = 1000\n",
    "MAX_STEP = 100000\n",
    "EPOCHS = TRAIN_LEN // BATCH_SIZE\n",
    "\n",
    "tfrecords_dir = './data/tfrecords'  #tfrecords目录\n",
    "log_dir = './logs'  # 保存参数日志的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename, batch_size, shuffle):  # 读取tfrecords数据\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                           'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "                                       })\n",
    "\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    img = tf.reshape(img, [N_H, N_W, N_C])\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # img = (img - 128) / 128.0\n",
    "    # img = tf.image.per_image_standardization(img)\n",
    "\n",
    "    if shuffle:\n",
    "        imgs, label_batch = tf.train.shuffle_batch(\n",
    "            [img, label],\n",
    "            batch_size=batch_size,\n",
    "            capacity=20000,\n",
    "            min_after_dequeue=3000)\n",
    "    else:\n",
    "        imgs, label_batch = tf.train.batch(\n",
    "            [img, label],\n",
    "            batch_size=batch_size,\n",
    "            capacity=20000)\n",
    "\n",
    "    label_batch = tf.one_hot(label_batch, depth=N_CLASS)\n",
    "    label_batch = tf.cast(label_batch, dtype=tf.int32)\n",
    "    label_batch = tf.reshape(label_batch, [batch_size, N_CLASS])\n",
    "\n",
    "    return imgs, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义多种网络结构\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, class_num, pool_size, is_train):\n",
    "        self.class_num = class_num\n",
    "        self.pool_size = pool_size\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def conv(self, x_tensor, conv_num_outputs, conv_ksize=3, conv_strides=1, conv_padding='SAME', name=None):\n",
    "        res = tf.layers.conv2d(x_tensor, conv_num_outputs, conv_ksize, strides=conv_strides, padding=conv_padding,\n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer(), name=name)\n",
    "        return res\n",
    "\n",
    "    def maxpool(self, x_tensor, pool_strides=(2, 2)):\n",
    "        res = tf.nn.max_pool(x_tensor, ksize=[1, self.pool_size[0], self.pool_size[1], 1],\n",
    "                             strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "        return res\n",
    "\n",
    "    def avgpool(self, x_tensor, pool_strides=(2, 2)):\n",
    "        res = tf.nn.avg_pool(x_tensor, ksize=[1, self.pool_size[0], self.pool_size[1], 1],\n",
    "                             strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "        return res\n",
    "\n",
    "    def fc(self, x_tensor, num_outputs, active=None, name=None):\n",
    "        std_dev = x_tensor.shape[-1].value ** -0.5\n",
    "        weight = tf.Variable(tf.random_normal([x_tensor.shape[-1].value, num_outputs], stddev=std_dev))\n",
    "        bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "        res = tf.add(tf.matmul(x_tensor, weight), bias, name=name)\n",
    "        if active == 'relu':\n",
    "            res = tf.nn.relu(res)\n",
    "        return res\n",
    "\n",
    "    def conv_with_batch_norm(self, X, size):\n",
    "        net = self.conv(X, size)\n",
    "        net = tf.layers.batch_normalization(net, training = self.is_train)\n",
    "        return net\n",
    "\n",
    "    def conv_relu(self, X, size):\n",
    "        net = self.conv(X, size)\n",
    "        net = tf.layers.batch_normalization(net, training=self.is_train)\n",
    "        net = tf.nn.relu(net)\n",
    "        return net\n",
    "\n",
    "    def basic_residual_block(self, X, size):\n",
    "        residual = tf.layers.conv2d(X, size, kernel_size = 1, padding='SAME')\n",
    "        net = self.conv_with_batch_norm(X, size)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = self.conv_with_batch_norm(net, size)\n",
    "        return residual + net\n",
    "\n",
    "    def basic_residual_block_3(self, X, size):\n",
    "        residual = tf.layers.conv2d(X, size, kernel_size=1, padding='SAME')\n",
    "\n",
    "        net = tf.layers.conv2d(X, size, kernel_size=1, padding='SAME')\n",
    "        net = tf.nn.relu(net)\n",
    "        net = self.conv_with_batch_norm(net, size)\n",
    "        net = tf.layers.conv2d(net, size, kernel_size=1, padding='SAME')\n",
    "        net = tf.nn.relu(net)\n",
    "        return residual + net\n",
    "\n",
    "    def residual_block(self, X, size, is_reduce=True):\n",
    "        net = self.basic_residual_block(X, size)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = self.basic_residual_block(net, size)\n",
    "        if is_reduce:\n",
    "            net = self.maxpool(net)\n",
    "        net = tf.nn.relu(net)\n",
    "        return net\n",
    "\n",
    "    def residual_block_3(self, X, size, is_reduce=True):\n",
    "        net = self.basic_residual_block_3(X, size)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = self.basic_residual_block_3(net, size)\n",
    "        if is_reduce:\n",
    "            net = self.maxpool(net)\n",
    "        net = tf.nn.relu(net)\n",
    "        return net\n",
    "\n",
    "\n",
    "    def Resnet_18(self, input_op):\n",
    "        net = self.conv(input_op, 64, name=\"input_node\")\n",
    "        net = self.maxpool(net)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = self.residual_block(net, 64)\n",
    "        net = self.residual_block(net, 128)\n",
    "#         net = self.residual_block(net, 256)\n",
    "        net = self.residual_block(net, 256, is_reduce=False)\n",
    "        net = self.avgpool(net)\n",
    "        net = tf.contrib.layers.flatten(net)\n",
    "        net = self.fc(net, 768, active = 'relu')\n",
    "        logits = self.fc(net, self.class_num, name=\"output_node\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def Resnet_50(self, input_op):\n",
    "        net = self.conv(input_op, 64, name=\"input_node\")\n",
    "        net = self.maxpool(net)\n",
    "        net = tf.nn.relu(net)\n",
    "        net = self.residual_block_3(net, 64)\n",
    "        net = self.residual_block_3(net, 128)\n",
    "        net = self.residual_block_3(net, 256)\n",
    "        net = self.residual_block_3(net, 512, is_reduce=False)\n",
    "        net = self.avgpool(net)\n",
    "        net = tf.contrib.layers.flatten(net)\n",
    "        net = self.fc(net, 512, active = 'relu')\n",
    "        logits = self.fc(net, self.class_num, name=\"output_node\")\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    tra_image_batch, tra_label_batch = read_and_decode(filename=os.path.join(tfrecords_dir, 'train.tfrecords'),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       shuffle=True)\n",
    "    val_image_batch, val_label_batch = read_and_decode(filename=os.path.join(tfrecords_dir, 'test.tfrecords'),\n",
    "                                                       batch_size=TEST_LEN,\n",
    "                                                       shuffle=True)\n",
    "\n",
    "    log = open('./log.txt', 'w')\n",
    "\n",
    "\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, N_H, N_W, N_C])\n",
    "    Y = tf.placeholder(tf.float32, [None, N_CLASS])\n",
    "    Z = tf.placeholder(tf.bool, name='training')\n",
    "\n",
    "    # to choose the nueral network: Resnet_18, Resnet_50\n",
    "    model = Model(N_CLASS, pool_size, Z)\n",
    "    logits = model.Resnet_18(X)\n",
    "\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learn_rate).minimize(cost) #RMSPropOptimizer.AdamOptimizer\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    try:\n",
    "        for step in np.arange(MAX_STEP):\n",
    "            if coord.should_stop():\n",
    "                break\n",
    "\n",
    "            tra_images, tra_labels = sess.run([tra_image_batch, tra_label_batch])\n",
    "            feed = {X: tra_images,\n",
    "                    Y: tra_labels,\n",
    "                    Z: 1}\n",
    "            _, tra_loss = sess.run([optimizer, cost], feed)\n",
    "            if step % 10 == 0 or (step + 1) == MAX_STEP:\n",
    "                val_images, val_labels = sess.run([val_image_batch, val_label_batch])\n",
    "                val_loss = sess.run(cost, feed_dict={X: val_images,\n",
    "                                                     Y: val_labels,\n",
    "                                                     Z: 1})\n",
    "                print('Step: %d, tra_loss: %.8f, val_loss: %.8f' % (step, tra_loss, val_loss))\n",
    "                print('Step: %d, tra_loss: %.8f, val_loss: %.8f' % (step, tra_loss, val_loss), file=log)\n",
    "\n",
    "            if step % EPOCHS == 0 or (step + 1) == MAX_STEP:\n",
    "                checkpoint_path = os.path.join(log_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
